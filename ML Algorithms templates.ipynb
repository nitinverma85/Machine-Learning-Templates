{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithms templates\n",
    "\n",
    "#### Basics\n",
    "1. If we plot errors (example mean squared) for different models, the model for which error is minimum is the best model. We find the least error using gradient descent method. \n",
    "2. In gradient method, you can decrease the parameter of the model by the learning rate each time to find the global minima of the cost function\n",
    "\n",
    "#### Loss function optimisation techniques\n",
    "1. Loss function : A function that calculates the error of your model. One example is Mean squared error which is typically used for Linear regression. \n",
    "2. Gradient Descent: We start with a set of random hyperparameters and then gradually decrease the loss function by calculating the gradient (derivative of loss function) w.r.t each hyperparameter. We try to move towards a gradient of 0 by moving the hyperparameter by a value equal to the learning rate. \n",
    "3. Stochastic Gradient Descent : Instead of passing all data to calculate the loss function, we pass a single observation to save time and compute instances. If we pass a small batch, its called Mini-batch stochastic gradient descent. SGD makes sequential passes over the training data, and during each pass, updates feature weights one example at a time with the aim of approaching the optimal weights that minimize the loss.\n",
    "\n",
    "\n",
    "#### Different types of ML\n",
    "1. Supervised: Labels are available. Further divided into regression and classification.Further divided into binary and multiclass classification\n",
    "2. Unsupervised: Labels are not available. Usually used for grouping information together. Further divided into Clustering, Dimensionality reduction and group words that have a similar meaning together. \n",
    "3. Reinforcement: We don’t give computers labels but want supervised answers. So, we give computers features and outcomes that would produce best answers. For example, if we are playing a game of chess, if we give data about what winners and losers did and whether they lost or won, it would be supervised learning. But if we tell the computer what winning and losing means, and give them lots of instances to train on, it would be reinforcement learning. \n",
    "\n",
    "#### Steps of ML\n",
    "1. Frame the core ML problem i.e what are you trying to predict?\n",
    "2. Collect data\n",
    "3. Analyze data\n",
    "4. Missing values\n",
    "5. Balanced classes in label/target variable?\n",
    "6. Variable target correlations (We might be inclined to include variables that have a high correlation with the target label)\n",
    "7. Feature engineering\n",
    "8. Feature selection\n",
    "9. Train the model\n",
    "10. Split data into test/train OR test/train/validation\n",
    "11. Validation data : Not seen by the model during training but indirectly used by the model to tune hyperparameters\n",
    "12. In a 10 fold cross validation, we divide the training data into 10 parts, 9 are used to train and 10th is used as validation data. If we continue this for 100 epochs, the 10th validation data is picked up randomly in each epoch. Do, validation data is picked up randomly from testing data itself, no need to split the data into validation dataset separately. \n",
    "13. Ensure both test and train data are random\n",
    "14. Evaluate and improve model accuracy\n",
    "15. Deploy the model\n",
    "\n",
    "#### Handling missing data\n",
    "\n",
    "1. Drop instances/observations using dropna method. Good for independent observations. eg. looking at car traffic of different suburbs\n",
    "2. Replace na with mean of the respective column\n",
    "3. Replace na with 0 \n",
    "4. Replace na with mean\n",
    "5. Replace na with interpolation (connect with last known values in the index) good for time-series data eg. looking at car traffic within a suburb at different hours of a day \n",
    "6. Replace na with forward fill (fill data of last observation with future observation)\n",
    "7. Replace na with backward fill\n",
    "8. Replace missing values with an average of values when grouped by values. Eg. looking at car traffic of different suburbs, we can replace missing values by mean of the traffic of respective suburb\n",
    "\n",
    "#### Handling numerical data\n",
    "\n",
    "1. Can be used as is\n",
    "2. Normalization/Scaling: Subtracting mean from each observation and dividing by the standard deviation. Typically used when the features are in a  different range. Features with large magnitude can start dominating the outcome without normalization. \n",
    "3. Binning: In many cases, the relationship between a numeric feature and the target is not linear (the feature value does not increase or decrease monotonically with the target). In such cases, it might be useful to bin the numeric feature into categorical features representing different ranges of the numeric feature. Each categorical feature (bin) can then be modeled as having its own linear relationship with the target. For example, say you know that the continuous numeric feature age is not linearly correlated with the likelihood to purchase a book. You can bin age into categorical features that might be able to capture the relationship with the target more accurately. The optimum number of bins for a numeric variable is dependent on characteristics of the variable and its relationship to the target, and this is best determined through experimentation. \n",
    "4. Cartesian features\n",
    "5. Combining features. For eg, you have length, breadth, and height as separate variables; you can create a new volume feature to be a product of these three variables\n",
    "6. Clipping: In case of outliers, clip them to a particular value \n",
    "\n",
    "\n",
    "#### Handling categorical data\n",
    "\n",
    "1. One hot encoding\n",
    "2. Tree-based algorithms like XG Boost can handle categorical values if they are numeric, so no one-hot encoding required. So, for example, if one of the features is ‘day of week’, tree based argos can simply be used after converting ‘Mon’ to 1, ‘Tue’ to 2 and so on. \n",
    "3. Some algorithms like Linear regression doesn’t handle like XG Boost, so One hot encoding is required\n",
    "4. Cartesian transformations: Combine categorical features into new features \n",
    "\n",
    "\n",
    "#### Overfitting\n",
    "1. Decrease features\n",
    "2. Introduce regularisation\n",
    "\n",
    "\n",
    "#### Underfitting\n",
    "1. Introduce more features\n",
    "2. Introduce cartesian products\n",
    "3. Change the type of feature processing eg. change n-grams size\n",
    "4. Decrease the amount of regularisation used\n",
    "\n",
    "\n",
    "#### Importing data\n",
    "\n",
    "\n",
    "#### Regularization\n",
    "\n",
    "Regularisation is needed when we have overfitting. When we have overfitting, we can use either Ridge or Lasso regression which alters the cost function and adds another slope element in the cost function(it penalises higher slopes or coefficients). So, the new model tries to push coefficients of variables to 0 (or remove certain variables) in order to minimise cost function.  \n",
    "\n",
    "- New cost function is (x-mean(x))^2 + lambda*(Slope)^2 for Ridge regression (L2 regularisation)\n",
    "- New cost function is (x-mean(x))^2 + lambda*mod(Slope) for Lasso regression (L1 regularisation)\n",
    "\n",
    "Since penalisation is higher in Lasso regression, the slopes of many features to 0. This also helps in feature selection.\n",
    "\n",
    "\n",
    "1. It is the technique of desensitizing your data to a particular dimension. It is applied when our model overfits. Regularisation is achieved through regression (implementation is via regression). \n",
    "2. L1 regularization (Lasso regression) has the effect of reducing the number of features used in the model by pushing to zero the weights of features that would otherwise have small weights. Used when we use feature crossing between 2 features with loads of values which generates a huge amount of values, all of which might not be important.\n",
    "3. L2 regularisation (Ridge regression) : Unlike L1 regularisation which pushes the weights of the coefficients to 0, this just pushes it down, not zero.  We want the weights of each feature to be nearly equal.So, add a term for weights (multiplied by a constant which defines a weight for this, it's called lambda) in the loss function and penalise if the weights are too large.The weights added are a sum of squares of individual weights.When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit:L2 regularization encourages weights to be small, but doesn't force them to exactly 0.0.If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn enough about the training data to make useful predictions.If your lambda value is too low, your model will be more complex, and you run the risk of overfitting your data. Your model will learn too much about the particularities of the training data, and won't be able to generalize to new data.\n",
    "4. The 3rd option to regularise a model is Early stopping, that is, limiting the number of training steps or the learning rate.\n",
    "5. Regularization is extremely important in logistic regression modeling.Imagine that you assign a unique id to each example, and map each id to its own feature. If you don't specify a regularization function, the model will become completely overfit. That's because the model would try to drive loss to zero on all examples and never get there, driving the weights for each indicator feature to +infinity or -infinity. \n",
    "\n",
    "#### Cross Validation\n",
    "\n",
    "- Number of combinations the data is split into test and training sets (since data might not be random)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  Price  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sklearn\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "df=pd.DataFrame(data=load_boston().data,columns=load_boston().feature_names)\n",
    "df['Price']=load_boston().target\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-12.46030057 -26.04862111 -33.07413798 -80.76237112 -33.31360656]\n",
      "Mean calculated using method 1 is -37.13180746769922\n",
      "Mean Calculated using method 2 is 21.894831181729213\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#sns.pairplot(data=df) RM and Lstat look important\n",
    "\n",
    "\n",
    "LinearRegressor=LinearRegression()\n",
    "LinearRegressor.fit(X,y)\n",
    "\n",
    "mse=cross_val_score(LinearRegressor,X,y,cv=5,scoring='neg_mean_squared_error')\n",
    "print(mse)\n",
    "print(\"Mean calculated using method 1 is {}\".format(np.mean(mse)))\n",
    "\n",
    "y_pred=LinearRegressor.predict(X)\n",
    "print(\"Mean Calculated using method 2 is {}\".format(((y_pred-y)**2).mean()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge and Lasso regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 150.20000000000002}\n",
      "-29.753615313588604\n",
      "24.292860642591133\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "ridge=Ridge()\n",
    "parameters={'alpha':.1*np.arange(2000)}\n",
    "RidgeRegressor=GridSearchCV(ridge,parameters,cv=5,scoring='neg_mean_squared_error' )\n",
    "RidgeRegressor.fit(X,y)\n",
    "print(RidgeRegressor.best_params_)\n",
    "print(RidgeRegressor.best_score_)\n",
    "y_pred=RidgeRegressor.predict(X)\n",
    "print(((y_pred-y)**2).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4967.024500395999, tolerance: 3.9191485420792076\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4503.78669191058, tolerance: 3.3071316790123455\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4291.079974925844, tolerance: 2.813643886419753\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2615.806563370839, tolerance: 3.3071762123456794\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:531: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4604.280842056452, tolerance: 3.4809104444444445\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.30000000000000004}\n",
      "-34.45504243875091\n",
      "23.58386889714523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lasso=Lasso()\n",
    "parameters={'alpha':.1*np.arange(2000)}\n",
    "LassoRegressor=GridSearchCV(lasso,parameters,cv=5,scoring='neg_mean_squared_error' )\n",
    "LassoRegressor.fit(X,y)\n",
    "print(LassoRegressor.best_params_)\n",
    "print(LassoRegressor.best_score_)\n",
    "y_pred=LassoRegressor.predict(X)\n",
    "print(((y_pred-y)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'50_Startups.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f9d1ba56d2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'50_Startups.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'50_Startups.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('50_Startups.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "labelEncoder=LabelEncoder()\n",
    "df['State']=labelEncoder.fit_transform(df['State'])\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "X_train,X_test,y_train, y_test=train_test_split(X,y,test_size=.3, random_state=0)\n",
    "LinearRegressor=LinearRegression()\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "LinearRegressor.fit(X_train,y_train)\n",
    "y_pred=(LinearRegressor.predict(X_test))\n",
    "\n",
    "df1=pd.DataFrame(X_test.copy())\n",
    "df1['Predicted profit']=y_pred\n",
    "df1['Actual Profit']=y_test\n",
    "df1['Difference(%)']=((y_pred-y_test)/y_test)*100\n",
    "df1.to_csv('predicted_values.csv')\n",
    "# df1.head(50)\n",
    "\n",
    "pd.DataFrame(LinearRegressor.coef_).to_csv('Coef.csv')\n",
    "print(LinearRegressor.intercept_)\n",
    "print(LinearRegressor.score(X_test,y_test))\n",
    "\n",
    "# plt.scatter(x=X_test['R&D Spend'],y=y_pred)\n",
    "# plt.scatter(x=X_test['R&D Spend'],y=y_test)\n",
    "# plt.xlabel('R&D spend')\n",
    "# plt.ylabel('Profit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance : Classification\n",
    "\n",
    "#### Confusion matrix\n",
    "\n",
    "- Columns represent predicted and have Yes and No\n",
    "- Rows represent actuala and have Yes and No\n",
    "\n",
    "- 1: True Positive\n",
    "- 2: False Positive (Type 1 error)\n",
    "- 3: False Negative (Type 2 error)\n",
    "- 4: True Negative\n",
    "\n",
    "\n",
    "Metrics\n",
    "\n",
    "- Accuracy: Right/Total: (1+4)/(1+2+3+4)\n",
    "- Recall or True Positive Rate (TPR)  : 1/(1+3)\n",
    "- Precision : 1/(1+2) \n",
    "- F1 score : 2*Precision*Recall/(Precision+Recall)\n",
    "- False Positive Rate (FPR) : 2/(2+4)\n",
    "- AUC (Area under curve) : Area under a curve drawn for all threshold values with FPR on X axis and TPR on y axis. The curve is called ROC curve. \n",
    "\n",
    "- AUC should be at least 50% otherwise it is a dumb model\n",
    "- AUC curve tells us what should be the threshhold value. It depends on whether we want to focus on maximising TPR or FNR. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0]\n",
      " [0 2 2]\n",
      " [1 0 1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.67      0.67      0.67         3\n",
      "           D       0.50      0.67      0.57         3\n",
      "           H       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.56         9\n",
      "   macro avg       0.56      0.56      0.55         9\n",
      "weighted avg       0.56      0.56      0.55         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_actual=['C','C','C','D','D','D','H','H','H']\n",
    "y_pred=['C','C','H','D','D','C','H','D','D']\n",
    "\n",
    "print(metrics.confusion_matrix(y_pred,y_actual)) #Order is Cat,Dog,Horse\n",
    "print(metrics.classification_report(y_actual,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics regression\n",
    "\n",
    "To classify a value in class 1 or class 2, we again find the best line which has the least cost function. \n",
    "\n",
    "- Cost function is different compared to linear regression\n",
    "- We maximise the cost function which is sum of f(+1(for class 1) and -1(for class 2) * Distance from best fit line)\n",
    "- f is the Sigmoid function here so that cost function of all values stay between 0 and 1 to negate the affect of outliers \n",
    "\n",
    "\n",
    "1. Logistic regression returns a probability. You can use the returned probability \"as is\" (for example, the probability that the user will click on this ad is 0.00023) or convert the returned probability to a binary value (for example, this email is spam).\n",
    "2. A logistic regression model that returns 0.9995 for a particular email message is predicting that it is very likely to be spam. Conversely, another email message with a prediction score of 0.0003 on that same logistic regression model is very likely not spam. However, what about an email message with a prediction score of 0.6? In order to map a logistic regression value to a binary category, you must define a classification threshold (also called the decision threshold). A value above that threshold indicates \"spam\"; a value below indicates \"not spam.\" It is tempting to assume that the classification threshold should always be 0.5, but thresholds are problem-dependent, and are therefore values that you must tune.\n",
    "3. We can use linear regression for classification problems but the results will be average  if there is an outlier, since the best fit line can be shifted which can cause a very bad result.   \n",
    "4. If there is too much overlapping on points in different classes, it's better not to use logistic regression because losgistic regression uses sigmoid function which classifies a  label as 1 or 0 abruptly (linear separation). If there's overlap, we would need some logic to classify overlapping points. So, for those purposes, other algos like decision trees,random forest, xg boost, svm, knn, aadboost etc.That's why linear classification problems are solved by Logistic regression while non linear classification problems are solved by non linear classification algorithms\n",
    "5. For multiclass lassification, it basically applies logistic regression multiple times assuming 2 classes at a time (combines all but one class into 1) and forms multiple models (equal to the number of classes). The one that gives the highest probability for a given class is the winner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    c1   c2   c3   c4  Flower\n",
      "0  5.1  3.5  1.4  0.2       0\n",
      "1  4.9  3.0  1.4  0.2       0\n",
      "2  4.7  3.2  1.3  0.2       0\n",
      "3  4.6  3.1  1.5  0.2       0\n",
      "4  5.0  3.6  1.4  0.2       0\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2 1 1 2 0 2 0 0]\n",
      "0.9777777777777777\n",
      "Precision is 0.9722222222222222\n",
      "Recall is 0.9814814814814815\n",
      "Accuracy is 0.9777777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "df=pd.DataFrame(iris.data,columns=['c1','c2','c3','c4'])\n",
    "df['Flower']=iris.target\n",
    "print(df.head())\n",
    "X=df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.3,random_state=0)\n",
    "logisticRegression=LogisticRegression()\n",
    "logisticRegression.fit(X_train,y_train)\n",
    "y_pred=logisticRegression.predict(X_test)\n",
    "\n",
    "print(y_pred)\n",
    "\n",
    "print(logisticRegression.score(X_test,y_test))\n",
    "\n",
    "print(\"Precision is {}\".format(sklearn.metrics.precision_score(y_test, y_pred, average = \"macro\")))\n",
    "print(\"Recall is {}\".format(sklearn.metrics.recall_score(y_test, y_pred, average = \"macro\")))\n",
    "print(\"Accuracy is {}\".format(sklearn.metrics.accuracy_score(y_test, y_pred)))\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Efficiency: Initial nodes (features) in the tree is selected in such a manner that the nodes are pure (Example, For a binary classification, child nodes select a feature that preferably splits the data into a single class (so that depth is low and the tree is less complex). Pureness can be calculated using either 'Entropy/Information gain' (lower entropy is better) or 'Gini' methodology (goes as a config in the algorithm). \n",
    "\n",
    "2. Applied for both regression and classification. Classification : Creates a decision tree and selects questions (nodes) in such a way that leaf nodes contain one of the classes. The topmost (parent root) is the feature that has the highest correlation with the label. Regression : Creates a decision tree by grouping instances which are similar and whose targets are close to each other. Predicted value is the avg of all instances that fall under that leaf node.\n",
    "\n",
    "3. (+) : Can handle non linear Datasets (unlike logistic regression), can handle numerical categorical features without one hot encoding. \n",
    "\n",
    "4. (-) Can grow deep and quickly overfit. Possible solution is to limit the depth of the tree but then it can underfit. Can be sometimes inaccurate and this led to Random forests. \n",
    "\n",
    "5. Tree-based algorithms like XG Boost etc can handle categorical values if they are numeric, so no one-hot encoding required. So, for example, if one of the features is ‘day of week’, tree based argos can simply be used after converting ‘Mon’ to 1, ‘Tue’ to 2 and so on. Some algorithms like Linear regression doesn’t handle like XG Boost, so One hot encoding is required\n",
    "\n",
    "6. Hyperparameters : max_depth (more depth means more complex model and can lead to overfitting, while less depth can cause underfitting)\n",
    "\n",
    "7. Hyperparametrs: min_samples_split/min_samples_leaf (what are the min number of samples available for splitting in each internal node or leaf node), default is 2, and if you make it 50, then a node, once it has 45 samples remaining, will not go any further (helps limit complexity of the model)\n",
    "\n",
    "8. Criterion: function used for splitting (Entropy or Gini)\n",
    "\n",
    "9. Hyperparameters: max_features: How many features to select when deciding which one should be the first node and subsequent nodes.  \n",
    "\n",
    "10. Decision trees don't require scaling of parameters. Because scaling has no impact on accuracy of decision trees. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decisionTreeClassifier=DecisionTreeClassifier()\n",
    "decisionTreeClassifier.fit(X_train,y_train)\n",
    "\n",
    "print(decisionTreeClassifier.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest neighbours\n",
    "\n",
    "1. Find which class the nearest k values lie in and decide\n",
    "2. Does not work well when the dataset is inbalanced (THINK WHY)\n",
    "3. Does not work well if you take outliers into account (THINK WHY)\n",
    "4. Works well when the data is not linear (unlike logistic regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df=pd.read_csv('classified_data.csv',index_col=0)\n",
    "standardScaler=StandardScaler()\n",
    "df_1=df.drop('TARGET CLASS',axis=1)\n",
    "df_1=pd.DataFrame(standardScaler.fit_transform(df_1),columns=df_1.columns)\n",
    "X_train,X_test,y_train, y_test=train_test_split(df_1,df['TARGET CLASS'],test_size=.3, random_state=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred=knn.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,y_pred)) \n",
    "print(metrics.classification_report(y_test,y_pred))\n",
    "\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate=[]\n",
    "for i in range(1,40):\n",
    "    knn=KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred=knn.predict(X_test)\n",
    "    error_rate.append(1-sklearn.metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,40),error_rate,marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble algorithms\n",
    "\n",
    "1. Ensemble means to combine multiple models\n",
    "\n",
    "#### Bagging and Boosting\n",
    "1. Bagging: Divides rows and columns into groups, with some instances (rows) and features (columns) in each group of data. Different models (decision trees in case of Random Forest and XGBoost) produce different predictions for each of these groups (different train data and the same test data). The average or highest frequency is taken when predicting new values. This helps solve the overfitting problem of decision trees since the tree no longer depends on all columns and all instances. \n",
    "2. Boosting: Initially, all rows(instances) are given equal weights. Instances of Incorrect prediction by the first sub model are given a higher weight and passed on the 2nd model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "1. Ensemble algorithm (Bagging method) where models that are used are decision trees. \n",
    "2. If we want to predict regression problems, then we use mean/median of all values obtained from different decision trees. \n",
    "3. Hyperparameter include how many models do you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling imbalanced datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say class 1 has 1mn datapoints and class 2 has 1k data points\n",
    "\n",
    "- Option 1: Undersampling - Pick 1k random datapoints from 1 mn for class 1. Not good since information is being thrown. \n",
    "- Option 2: Oversampling - Increase 1k datapoints to 1mn for class 2 . \n",
    "- Option 3: Oversampling using SMOTE. Use K Neaest to find more samples of minority class near existing samples. SMOTE is Synthetic minority oversampling technique\n",
    "- Option 4: Divide 1mn into 100 datasets of 1k each. Then build 100 models and use majority vote after predicting from all those 100 models. \n",
    "- Option 5: Focal loss: Penalise majority class and give more weightage to the minority class. \n",
    "\n",
    "\n",
    "Library used for undersampling: from imblearn.under_sampling import NearMiss\n",
    "Library used for oversampling\n",
    "- from imblearn.combine import SMOTETomek\n",
    "- from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "Oversampling is better than undersampling because we are not losing any information here, unlike undersampling\n",
    "\n",
    "###### How to measure performance of imbalanced datasets\n",
    "- Accuracy is a wrong measure since accuracy will be high even if the model predicts all as majority class\n",
    "- So, instead we look at f1 scores of individual classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "Hyperparameters of XGBoost\n",
    "\n",
    "- N_estimators : The number of decision trees to be included in the analysis\n",
    "- Max_depth : Depth of one tree in XGBoost, max_depth (more depth means more complex model and can lead to overfitting, while less depth can cause underfitting), Range 2-30\n",
    "- min_samples_split/min_samples_leaf (what are the min number of samples available for splitting in each internal node or leaf node), default is 2, and if you make it - then a node, once it has 45 samples remaining, will not go any further (helps limit complexity of the model)\n",
    "- Max_features: How many features to select when deciding which one should be the first node and subsequent nodes\n",
    "- Subsample (% of row instances to be used), Col sample by tree, Col sample by level (number of columns to be used), decreasing it reduces overfitting \n",
    "- Learning rate: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('Churn_Modelling.csv')\n",
    "X=df.iloc[:,3:13]\n",
    "y=df.iloc[:,13]\n",
    "print(df.columns)\n",
    "print(X.columns)\n",
    "df.head()\n",
    "\n",
    "categorical_features = [feature for feature in X.columns if X[feature].dtype=='O']\n",
    "print(categorical_features)\n",
    "Geography=pd.get_dummies(X['Geography'],drop_first=True)\n",
    "Gender=pd.get_dummies(X['Gender'],drop_first=True)\n",
    "X=pd.concat([X,Gender,Geography],axis=1)\n",
    "X=X.drop(['Geography','Gender'],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df.corr(),annot=True,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "params = {\n",
    "    \"learning_rate\": .05*np.arange(1,10),\n",
    "    \"max_depth\": np.arange(3,15),\n",
    "    \"min_child_weight\": np.arange(3,10),\n",
    "    \"gamma\": .1*np.arange(0,10),\n",
    "    \"colsample_bytree\": .1*np.arange(3,10)\n",
    "}\n",
    "import xgboost\n",
    "classifier = xgboost.XGBClassifier()\n",
    "classifier_1=RandomizedSearchCV(classifier, param_distributions=params)\n",
    "\n",
    "classifier_1.fit(X,y)\n",
    "y_pred=classifier_1.predict(X)\n",
    "\n",
    "print(classifier_1.best_params_)\n",
    "print(\"Accuracy is {}\".format(accuracy_score(y,y_pred)))\n",
    "print(pd.crosstab(y,y_pred))\n",
    "print(classification_report(y,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    "\n",
    "Step 1 : Create a decision tree by selecting one column (whichever has the lowest entropy or Gini value)\n",
    "\n",
    "Step 2: Calculate total weighted error. Initially all records have equal weights. \n",
    "\n",
    "Step 3: Update the weights and give the rows (instances) that had predicted wrong values a higher weight. \n",
    "\n",
    "Step 4: Next time, instances are picked according to weights. So, chances of wrong records egtting selected are high. \n",
    "\n",
    "\n",
    "Step 5: Repeat this process and take majority value obtained from all the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter optimisation\n",
    "\n",
    "1. GridSearchCV or RandomisedSearchCV : 2 options for providing multiple hyperparameters to models\n",
    "\n",
    "2. GridSearchCV tests all possible combinations of all parameters, so is better but slow. RandomisedSearchCV uses the best possible combination within a specifed number of iterations (there is a config called n_iter in RandomisedSearchCV). \n",
    "\n",
    "3. Process : Check all hyperparameters of the model, use GridSearchCV and RandomisedSearchCV to tune the hyperparameters. Use RandomisedSearch CV first and then pass output of RandomisedSearchCV (with hyperparameter in vicinity of values around those) to GridSearchCV. \n",
    "\n",
    "4. Automated hyperparameter tuning using Bayesian Optimization. In Bayesian Optimization, we try to minimise the loss function using probability. Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin. First is a space variable (hyperparameters), objective function (loss function) and an optimization algorithm (defines the search algorithm to use to select the best input values to use in each new iteration)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Why do we require scaling. \n",
    "\n",
    "2. Why don't decision trees require scaling "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
